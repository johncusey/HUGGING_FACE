{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu5cjDiuyeulclWywggvI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b18e3647e4a44c9ebcbd737df5c33b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a5b241a05c642239ec66c302a950ea7",
              "IPY_MODEL_35425a5ab78643d7ac4df1f3033ef0a2",
              "IPY_MODEL_41d46977ccd84c64b6ccddcae9f21d53"
            ],
            "layout": "IPY_MODEL_8558dd272d1f45df9c8f7d63c3d9d461"
          }
        },
        "8a5b241a05c642239ec66c302a950ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f08e4db25b7e4d2485147cb4770f8d59",
            "placeholder": "​",
            "style": "IPY_MODEL_46a8e475ac8549d896fdf3dc4c7cf33a",
            "value": "aya-23-8B-IQ2_M.gguf: 100%"
          }
        },
        "35425a5ab78643d7ac4df1f3033ef0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e84855e8044544bc97f4674a37331535",
            "max": 3084807296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fb025b2c59d419cbd3f7a5450868ef2",
            "value": 3084807296
          }
        },
        "41d46977ccd84c64b6ccddcae9f21d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a0be4b18075444894c5a915065bcb08",
            "placeholder": "​",
            "style": "IPY_MODEL_c0e4728d92714a4cbffb3e44e7bfc6db",
            "value": " 3.08G/3.08G [00:41&lt;00:00, 87.3MB/s]"
          }
        },
        "8558dd272d1f45df9c8f7d63c3d9d461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08e4db25b7e4d2485147cb4770f8d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a8e475ac8549d896fdf3dc4c7cf33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e84855e8044544bc97f4674a37331535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb025b2c59d419cbd3f7a5450868ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a0be4b18075444894c5a915065bcb08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0e4728d92714a4cbffb3e44e7bfc6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johncusey/HUGGING_FACE/blob/main/Analysis_Customer_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afgbWULcQoca",
        "outputId": "69966ede-5132-4032-a1b6-26eb2648934c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.3.2\n",
            "  Downloading llama_cpp_python-0.3.2.tar.gz (65.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.2) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.2) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.2) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.2) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.2-cp311-cp311-linux_x86_64.whl size=3410036 sha256=8e5081132cafaea817f23881443b5385188fcaa6892350a6beb5df88c892fcd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/23/53/f88ac1148db83642747c5989bb9d32cc2de2fa8e54c4c2eb7f\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python==0.3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "LL7ikKTrSRXf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"bartowski/aya-23-8B-GGUF\",\n",
        "    filename=\"aya-23-8B-IQ2_M.gguf\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b18e3647e4a44c9ebcbd737df5c33b1c",
            "8a5b241a05c642239ec66c302a950ea7",
            "35425a5ab78643d7ac4df1f3033ef0a2",
            "41d46977ccd84c64b6ccddcae9f21d53",
            "8558dd272d1f45df9c8f7d63c3d9d461",
            "f08e4db25b7e4d2485147cb4770f8d59",
            "46a8e475ac8549d896fdf3dc4c7cf33a",
            "e84855e8044544bc97f4674a37331535",
            "1fb025b2c59d419cbd3f7a5450868ef2",
            "9a0be4b18075444894c5a915065bcb08",
            "c0e4728d92714a4cbffb3e44e7bfc6db"
          ]
        },
        "id": "RZQNZpBkSVJE",
        "outputId": "efc5adbb-fa94-4bf6-fb8e-9d6b7529fcf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "aya-23-8B-IQ2_M.gguf:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b18e3647e4a44c9ebcbd737df5c33b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 32 key-value pairs and 258 tensors from /root/.cache/huggingface/hub/models--bartowski--aya-23-8B-GGUF/snapshots/faadc356505e86ffd2ce757ddc04e97fd33c1b30/./aya-23-8B-IQ2_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = command-r\n",
            "llama_model_loader: - kv   1:                               general.name str              = aya-23-8B\n",
            "llama_model_loader: - kv   2:                      command-r.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                   command-r.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                 command-r.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:              command-r.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:             command-r.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:          command-r.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                   command-r.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   9:     command-r.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 29\n",
            "llama_model_loader: - kv  11:                      command-r.logit_scale f32              = 0.062500\n",
            "llama_model_loader: - kv  12:                command-r.rope.scaling.type str              = none\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = command-r\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,253333]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ a...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 5\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 255001\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:           tokenizer.chat_template.tool_use str              = {{ bos_token }}{% if messages[0]['rol...\n",
            "llama_model_loader: - kv  24:                tokenizer.chat_template.rag str              = {{ bos_token }}{% if messages[0]['rol...\n",
            "llama_model_loader: - kv  25:                   tokenizer.chat_templates arr[str,2]       = [\"tool_use\", \"rag\"]\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
            "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  28:                      quantize.imatrix.file str              = /models/aya-23-8B-GGUF/aya-23-8B.imatrix\n",
            "llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
            "llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 194\n",
            "llama_model_loader: - type  f32:   33 tensors\n",
            "llama_model_loader: - type q4_K:   32 tensors\n",
            "llama_model_loader: - type q5_K:    1 tensors\n",
            "llama_model_loader: - type iq3_s:   36 tensors\n",
            "llama_model_loader: - type iq2_s:  156 tensors\n",
            "llm_load_vocab: control token: 255001 '<|END_OF_TURN_TOKEN|>' is not marked as EOG\n",
            "llm_load_vocab: control token:      7 '<EOP_TOKEN>' is not marked as EOG\n",
            "llm_load_vocab: control token:      2 '<CLS>' is not marked as EOG\n",
            "llm_load_vocab: control token:      3 '<SEP>' is not marked as EOG\n",
            "llm_load_vocab: control token:      6 '<EOS_TOKEN>' is not marked as EOG\n",
            "llm_load_vocab: control token:      1 '<UNK>' is not marked as EOG\n",
            "llm_load_vocab: control token:      4 '<MASK_TOKEN>' is not marked as EOG\n",
            "llm_load_vocab: control token:      5 '<BOS_TOKEN>' is not marked as EOG\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 1008\n",
            "llm_load_vocab: token to piece cache size = 1.8528 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = command-r\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 256000\n",
            "llm_load_print_meta: n_merges         = 253333\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 6.2e-02\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = none\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = IQ2_M - 2.7 bpw\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 2.86 GiB (3.06 BPW) \n",
            "llm_load_print_meta: general.name     = aya-23-8B\n",
            "llm_load_print_meta: BOS token        = 5 '<BOS_TOKEN>'\n",
            "llm_load_print_meta: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<PAD>'\n",
            "llm_load_print_meta: LF token         = 136 'Ä'\n",
            "llm_load_print_meta: FIM PAD token    = 0 '<PAD>'\n",
            "llm_load_print_meta: EOG token        = 0 '<PAD>'\n",
            "llm_load_print_meta: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'\n",
            "llm_load_print_meta: max token length = 1024\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q5_K) (and 258 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2931.52 MiB\n",
            "..............................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 512\n",
            "llama_new_context_with_model: n_ctx_per_seq = 512\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   508.00 MiB\n",
            "llama_new_context_with_model: graph nodes  = 968\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_data/calibration_data.txt', 'quantize.imatrix.chunks_count': '194', 'quantize.imatrix.file': '/models/aya-23-8B-GGUF/aya-23-8B.imatrix', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '255001', 'tokenizer.ggml.bos_token_id': '5', 'tokenizer.chat_template.rag': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = '## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.' %}{% endif %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ '# Safety Preamble' }}{{ '\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\'t answer questions that are harmful or immoral.' }}{{ '\\n\\n# System Preamble' }}{{ '\\n## Basic Rules' }}{{ '\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\'s requests, you cite your sources in your answers, according to those instructions.' }}{{ '\\n\\n# User Preamble' }}{{ '\\n' + system_message }}{{ '<|END_OF_TURN_TOKEN|>'}}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'system' %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'}}{{ '<results>' }}{% for document in documents %}{{ '\\nDocument: ' }}{{ loop.index0 }}\\n{% for key, value in document.items() %}{{ key }}: {{value}}\\n{% endfor %}{% endfor %}{{ '</results>'}}{{ '<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ 'Carefully perform the following instructions, in order, starting each with a new line.\\n' }}{{ 'Firstly, Decide which of the retrieved documents are relevant to the user\\\\'s last input by writing \\\\'Relevant Documents:\\\\' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\\\'None\\\\'.\\n' }}{{ 'Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\\\'s last input by writing \\\\'Cited Documents:\\\\' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\\\'None\\\\'.\\n' }}{% if citation_mode=='accurate' %}{{ 'Thirdly, Write \\\\'Answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\\n' }}{% endif %}{{ 'Finally, Write \\\\'Grounded answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.' }}{{ '<|END_OF_TURN_TOKEN|>' }}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'general.architecture': 'command-r', 'tokenizer.ggml.pre': 'command-r', 'general.name': 'aya-23-8B', 'tokenizer.ggml.add_eos_token': 'false', 'command-r.context_length': '8192', 'tokenizer.chat_template.tool_use': '{{ bos_token }}{% if messages[0][\\'role\\'] == \\'system\\' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][\\'content\\'] %}{% else %}{% set loop_messages = messages %}{% set system_message = \\'## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\\' %}{% endif %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' }}{{ \\'# Safety Preamble\\' }}{{ \\'\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\\\'t answer questions that are harmful or immoral.\\' }}{{ \\'\\n\\n# System Preamble\\' }}{{ \\'\\n## Basic Rules\\' }}{{ \\'\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\\\'s requests, you cite your sources in your answers, according to those instructions.\\' }}{{ \\'\\n\\n# User Preamble\\' }}{{ \\'\\n\\' + system_message }}{{\\'\\n\\n## Available Tools\\nHere is a list of tools that you have available to you:\\n\\n\\'}}{% for tool in tools %}{% if loop.index0 != 0 %}{{ \\'\\n\\n\\'}}{% endif %}{{\\'```python\\ndef \\' + tool.name + \\'(\\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\', \\'}}{% endif %}{{param_name}}: {% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\'] = None\\'}}{% else %}{{ param_fields.type }}{% endif %}{% endfor %}{{ \\') -> List[Dict]:\\n    \"\"\"\\'}}{{ tool.description }}{% if tool.parameter_definitions|length != 0 %}{{ \\'\\n\\n    Args:\\n        \\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\'\\n        \\' }}{% endif %}{{ param_name + \\' (\\'}}{% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\']\\'}}{% else %}{{ param_fields.type }}{% endif %}{{ \\'): \\' + param_fields.description }}{% endfor %}{% endif %}{{ \\'\\n    \"\"\"\\n    pass\\n```\\' }}{% endfor %}{{ \\'<|END_OF_TURN_TOKEN|>\\'}}{% for message in loop_messages %}{% set content = message[\\'content\\'] %}{% if message[\\'role\\'] == \\'user\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'system\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'assistant\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\'  + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% endif %}{% endfor %}{{\\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\\\\\'Action:\\\\\\' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\\\\\'s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\\n```json\\n[\\n    {\\n        \"tool_name\": title of the tool in the specification,\\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\\n    }\\n]```<|END_OF_TURN_TOKEN|>\\'}}{% if add_generation_prompt %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\' }}{% endif %}', 'command-r.attention.layer_norm_epsilon': '0.000010', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'command-r.logit_scale': '0.062500', 'command-r.rope.scaling.type': 'none', 'command-r.embedding_length': '4096', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'command-r.feed_forward_length': '14336', 'command-r.attention.head_count': '32', 'command-r.block_count': '32', 'command-r.attention.head_count_kv': '8', 'command-r.rope.freq_base': '10000.000000', 'general.file_type': '29'}\n",
            "Available chat formats from metadata: chat_template.rag, chat_template.tool_use, chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\n",
            "Using chat eos_token: <|END_OF_TURN_TOKEN|>\n",
            "Using chat bos_token: <BOS_TOKEN>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    \"This product is amazing! The quality exceeded my expectations, and the customer service was excellent. (English)\",  # Positive, highlights quality & service\n",
        "    \"Este producto es increíble. La entrega fue rápida y el empaque estaba bien protegido. (Spanish)\",  # Positive, mentions delivery & packaging\n",
        "    \"Ce produit est de bonne qualité, mais le prix est un peu trop élevé pour ce qu'il offre. (French)\",  # Mixed review, good quality but expensive\n",
        "    \"Dieses Produkt hat mich enttäuscht. Es funktioniert nicht wie erwartet und der Kundendienst reagiert langsam. (German)\",  # Negative, mentions performance & customer service\n",
        "    \"この製品にはいくつかの良い点がありますが、耐久性に問題があります。数週間で壊れてしまいました。 (Japanese)\",  # Negative, durability issue\n",
        "]\n",
        "\n",
        "\n",
        "prompt = (\n",
        "    \"<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\n\"\n",
        "    \"Below are multiple customer reviews in different languages. Please analyze them as a whole and provide:\\n\"\n",
        "    \"1. A general sentiment analysis of the reviews collectively.\\n\"\n",
        "    \"2. A summary of the key strengths customers appreciate.\\n\"\n",
        "    \"3. A business weakness that should be improved.\\n\"\n",
        "    \"Here are the reviews:\\n\\n\"\n",
        "    f\"{' '.join(reviews)}\\n\"\n",
        "    \"<|END_OF_TURN_TOKEN|>\"\n",
        ")\n",
        "\n",
        "response = llm.create_chat_completion (\n",
        "    messages=[\n",
        "        {\"role\":\"user\",\"content\":prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlNtmkX-U28T",
        "outputId": "da456b35-7e5b-4c1d-d42d-a9d36eced8d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =  295084.30 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  761821.54 ms /   496 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall sentiment: The reviews for this product across multiple languages are generally positive, with most customers giving positive feedback. \n",
            "\n",
            "Language by language, here is a breakdown:\n",
            "\n",
            "English: The reviews indicate a positive sentiment, with customers praising the product's quality and customer service. \n",
            "\n",
            "Spanish: The reviewer is happy with the product and its delivery, with the added bonus of being well-protected during shipment. \n",
            "\n",
            "French: While the product is of good quality, the reviewer feels the price is too high for what it offers. \n",
            "\n",
            "German: The customer is disappointed with the product's performance and the slow response from customer service. \n",
            "\n",
            "Japanese: The product is disappointing as it has a durability issue, with the product breaking within a few weeks. \n",
            "\n",
            "Key strengths that customers appreciate: \n",
            "\n",
            "- Quality of the product (English, Spanish, French, and Japanese)\n",
            "- Customer service (English)\n",
            "- Delivery and protection of the product during shipment (Spanish)\n",
            "\n",
            "Weakness for the business: \n",
            "\n",
            "Durability of the product and customer service response time (German and Japanese), with customers from different languages expressing these issues. \n",
            "\n",
            "A summary of the reviews: \n",
            "\n",
            "The product receives generally positive feedback, with customers praising the quality and delivery. However, there are concerns about the durability of the product, and the price could be better for what is offered. The German and Japanese reviews highlight the need for better customer service response times and product durability.\n"
          ]
        }
      ]
    }
  ]
}